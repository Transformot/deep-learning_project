{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a6ffb2c272a0f5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Concolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8756f5c6a206c30e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Importation librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import normalize, to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Activation, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras import models, callbacks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d38e41eb5da70",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Importation et affichage données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850501202a077edc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = np.loadtxt('data/ECG200_TRAIN.tsv', delimiter='\\t')\n",
    "test_data = np.loadtxt('data/ECG200_TEST.tsv', delimiter='\\t')\n",
    "\n",
    "# Afficher la première série temporelle\n",
    "print(\"health:\", train_data[0,0])\n",
    "print(train_data[0,1:])\n",
    "\n",
    "# Afficher les dimensions des données\n",
    "print(\"Dimensions des données d'entrainnement:\", train_data.shape)\n",
    "print(\"Dimensions des données de test:\", test_data.shape)\n",
    "\n",
    "# Afficher les 5 premières séries temporelles\n",
    "nb_series_a_afficher = 5\n",
    "plt.figure(figsize=(27, 7))\n",
    "for i in range(nb_series_a_afficher):\n",
    "    # Extraire la série temporelle et son état de santé correspondant\n",
    "    serie_temporelle = train_data[i, 1:]\n",
    "    etat_sante = train_data[i, 0]\n",
    "    \n",
    "    # Créer une plage de temps pour l'axe x\n",
    "    plage_temps = range(len(serie_temporelle))\n",
    "    \n",
    "    # Placer la série temporelle dans le sous-graphique correspondant\n",
    "    plt.subplot(2, nb_series_a_afficher, i+1)\n",
    "    plt.plot(plage_temps, serie_temporelle, label='Série temporelle')\n",
    "    plt.xlabel('Temps')\n",
    "    plt.ylabel('Valeurs')\n",
    "    plt.title(f'Série temporelle {i+1} (Santé: {\"Bon\" if etat_sante == -1 else \"Mauvais\"})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "# Ajuster automatiquement les espaces entre les sous-graphiques\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66daa99",
   "metadata": {},
   "source": [
    "## Traitement données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Normaliser les série temporelle entre 0 et 1 dépendament des autres\n",
    "X_train_normalized = (X_train - np.min(X_train)) / (np.max(X_train) - np.min(X_train))\n",
    "X_test_normalized = (X_test - np.min(X_test)) / (np.max(X_test) - np.min(X_test))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliser les classe\n",
    "train_data[train_data[:, 0] == -1, 0] = 0\n",
    "test_data[test_data[:, 0] == -1, 0] = 0\n",
    "\n",
    "# Séparation des données d'entrainnements et de tests\n",
    "X_train, y_train = train_data[:, 1:], train_data[:, 0]\n",
    "X_test, y_test = test_data[:, 1:], test_data[:, 0]\n",
    "\n",
    "# Normaliser les série temporelle entre 0 et 1 indépendament\n",
    "X_train_normalized = normalize(X_train, axis=1)\n",
    "X_test_normalized = normalize(X_test, axis=1)\n",
    "\n",
    "# Convertir les étiquettes en encodage one-hot\n",
    "\"\"\"y_train_encoded = to_categorical(y_train, num_classes=2)\n",
    "y_test_encoded = to_categorical(y_test, num_classes=2)\"\"\"\n",
    "\n",
    "# Afficher la première série temporelle\n",
    "print(\"health:\", y_train[0])\n",
    "print(X_train_normalized[0])\n",
    "print(\"Dimensions des données d'entrainnement:\", (X_train_normalized.shape, y_train.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52567e24",
   "metadata": {},
   "source": [
    "## Choix des Hyperparamètres\n",
    "\n",
    "❒ Les hyperparamètres des layers : kernel size, dropout, méthode d’activation des layers cachés, méthode d’activation du layer final, …\n",
    "\n",
    "❒ Les hyperparamètres de compilation du modèle: optimizer, loss, learning rate …\n",
    "\n",
    "❒ Les hyperparamètres d’exécution du modèle : batch size, nombre d’epochs, …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameter des layers :\n",
    "filters = 3\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "padding = 'same'\n",
    "hidden_activation = 'relu'\n",
    "final_activation = 'sigmoid' \n",
    "pool_size = 1\n",
    "dropout_rate = 0.3\n",
    "nb_classes = 1\n",
    "\n",
    "# HyperParameter de compilation : \n",
    "\"\"\"learning_rate = 0.001 \"\"\"                        # choose learning rate pour l'optimiseur adam\n",
    "optimizer_algo = 'adam'\n",
    "cost_function = 'binary_crossentropy'               # ou categorical_crossentropy\n",
    "\n",
    "## HyperParameter d’exécution : \n",
    "mini_batch_size = 16\n",
    "nb_epochs = 1500\n",
    "percentage_of_train_as_validation = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1df5f",
   "metadata": {},
   "source": [
    "## Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_shape = (96, 1)\n",
    "input_layer = Input(input_shape)\n",
    "\n",
    "# block\n",
    "conv_layer_1_1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding)(input_layer)\n",
    "relu_layer_1_1 = Activation(hidden_activation)(conv_layer_1_1)\n",
    "conv_layer_1_2 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding)(relu_layer_1_1)\n",
    "relu_layer_1_2 = Activation(hidden_activation)(conv_layer_1_2)\n",
    "pooling_layer_1 = MaxPooling1D(pool_size=pool_size,padding=padding)(relu_layer_1_2)\n",
    "dropout_layer_1_1 = Dropout(rate=dropout_rate)(relu_layer_1_1)\n",
    "\n",
    "# output\n",
    "flattened_layer = Flatten()(pooling_layer_1)\n",
    "dropout_flattened = Dropout(rate=dropout_rate)(flattened_layer)\n",
    "output_layer = Dense(units=nb_classes, activation=final_activation)(dropout_flattened)\n",
    "\n",
    "model = models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfef551",
   "metadata": {},
   "source": [
    "## Compilation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=cost_function,optimizer=optimizer_algo, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a38871",
   "metadata": {},
   "source": [
    "## Entrainnement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the model checkpoint (to save the best model for each epoch)\n",
    "model_checkpoint = callbacks.ModelCheckpoint('best_model_CNN.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# start training\n",
    "history = model.fit(X_train_normalized, y_train,\n",
    "                    batch_size=mini_batch_size, \n",
    "                    epochs=nb_epochs,\n",
    "                    validation_split=percentage_of_train_as_validation,\n",
    "                    verbose=False,\n",
    "                    callbacks=[model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a68424",
   "metadata": {},
   "source": [
    "## Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d58e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charge le meilleur modèle\n",
    "best_model = models.load_model('best_model_CNN.keras')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test_normalized, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "train_loss, train_accuracy = best_model.evaluate(X_train_normalized, y_train)\n",
    "\n",
    "print(f\"Test Loss: {train_loss}\")\n",
    "print(f\"Test Accuracy: {train_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e0af9",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fea15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465a802",
   "metadata": {},
   "source": [
    "### Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859856de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred = model.predict(X_test_normalized)\n",
    "\n",
    "# Convertir les probabilités de y_pred en classes binaires\n",
    "y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
    "\n",
    "# Afficher la matrice de confusion sous forme de heatmap\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "class_names = ['Positif', 'Négatif']\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac8ea0",
   "metadata": {},
   "source": [
    "## Recherche des hyperparamètre\n",
    "par recherche par grille et recherche aléatoire (à faire: avec de la validation croisé)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a187aed",
   "metadata": {},
   "source": [
    "## Meilleur modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f98c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparamètres\n",
    "filters = 5\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 'same'\n",
    "use_bias = False\n",
    "hidden_activation = 'relu'\n",
    "final_activation = 'sigmoid' \n",
    "pool_size = 2\n",
    "dropout_rate = 0.2\n",
    "nb_classes = 1\n",
    "# learning_rate = 0.006\n",
    "optimizer_algo = 'adam'  # Adam(learning_rate=learning_rate)\n",
    "cost_function = 'binary_crossentropy'\n",
    "mini_batch_size = 16\n",
    "nb_epochs = 800\n",
    "percentage_of_train_as_validation = 0.2\n",
    "\n",
    "# build and compil modelt\n",
    "input_shape = (96, 1)\n",
    "input_layer = Input(input_shape)\n",
    "conv_layer_1_1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(input_layer)\n",
    "relu_layer_1_1 = Activation(hidden_activation)(conv_layer_1_1)\n",
    "conv_layer_1_2 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(relu_layer_1_1)\n",
    "relu_layer_1_2 = Activation(hidden_activation)(conv_layer_1_2)\n",
    "pooling_layer_1 = MaxPooling1D(pool_size=pool_size, padding=padding)(relu_layer_1_2)\n",
    "dropout_layer_1_1 = Dropout(rate=dropout_rate)(pooling_layer_1)\n",
    "conv_layer_2_1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(dropout_layer_1_1)\n",
    "relu_layer_2_1 = Activation(hidden_activation)(conv_layer_2_1)\n",
    "conv_layer_2_2 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(relu_layer_2_1)\n",
    "relu_layer_2_2 = Activation(hidden_activation)(conv_layer_2_2)\n",
    "pooling_layer_2 = MaxPooling1D(pool_size=pool_size, padding=padding)(relu_layer_2_2)\n",
    "flattened_layer = Flatten()(pooling_layer_2)\n",
    "dropout_flattened = Dropout(rate=dropout_rate)(flattened_layer)\n",
    "output_layer = Dense(units=nb_classes,activation=final_activation)(dropout_flattened)\n",
    "model_cnn = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "model_cnn.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint('best_model_CNN.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# start training\n",
    "start_training = time.time()\n",
    "history = model_cnn.fit(X_train_normalized, y_train, \n",
    "                    batch_size=mini_batch_size, \n",
    "                    epochs=nb_epochs,\n",
    "                    validation_split=percentage_of_train_as_validation,\n",
    "                    verbose=False,\n",
    "                    callbacks=[model_checkpoint])\n",
    "end_training = time.time()\n",
    "\n",
    "# Eveluate best model\n",
    "best_model = models.load_model('best_model_CNN.keras')\n",
    "train_loss, train_accuracy = best_model.evaluate(X_train_normalized, y_train)\n",
    "start_evaluate = time.time()\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test_normalized, y_test)\n",
    "end_evaluate = time.time()\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model_cnn.predict(X_test_normalized)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "class_names = ['Positif', 'Négatif']\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "class_names = ['Positif', 'Négatif']\n",
    "\n",
    "# result\n",
    "plot_model(model_cnn, show_shapes=True)\n",
    "training_time_seconds = end_training - start_training\n",
    "evaluate_time_seconds = end_evaluate - start_evaluate\n",
    "print(f\"Nombre total de paramètres : {best_model.count_params()}\")\n",
    "print(f\"Temps d'entraînement : {training_time_seconds // 60} minutes et {training_time_seconds % 60} secondes.\")\n",
    "print(f\"Temps d'évaluation : {evaluate_time_seconds // 60} minutes et {evaluate_time_seconds % 60} secondes.\")\n",
    "print(f\"Test Loss : {test_loss}\")\n",
    "print(f\"Test Accuracy : {test_accuracy}\")\n",
    "print(f\"AUC-ROC : \", roc_auc)\n",
    "print(f\"Train Loss : {train_loss}\")\n",
    "print(f\"Train Accuracy : {train_accuracy}\")\n",
    "\n",
    "# Plot \n",
    "plt.figure(figsize=(16, 12))# mettre plus d'espace\n",
    "# Training and Validation Accuracy\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "# Training and Validation Loss\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "# Confusion Matrix\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "# ROC Curve\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0a399",
   "metadata": {},
   "source": [
    "## Validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d13a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt('data/ECG200_TRAIN.tsv', delimiter='\\t')\n",
    "test_data = np.loadtxt('data/ECG200_TEST.tsv', delimiter='\\t')\n",
    "\n",
    "test_data[test_data[:, 0] == -1, 0] = 0\n",
    "X_test, y_test = test_data[:, 1:], test_data[:, 0]\n",
    "X_test_normalized = normalize(X_test, axis=1)\n",
    "\n",
    "# hyperparamètres\n",
    "filters = 5\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 'same'\n",
    "use_bias = False\n",
    "hidden_activation = 'relu'\n",
    "final_activation = 'sigmoid' \n",
    "pool_size = 2\n",
    "dropout_rate = 0.2\n",
    "nb_classes = 1\n",
    "# learning_rate = 0.006\n",
    "optimizer_algo = 'adam'  # Adam(learning_rate=learning_rate)\n",
    "cost_function = 'binary_crossentropy'\n",
    "mini_batch_size = 16\n",
    "nb_epochs = 800\n",
    "percentage_of_train_as_validation = 0.2\n",
    "\n",
    "# Création de 5 ensembles de validation croisée car 20% de validation\n",
    "num_splits = 5\n",
    "test_accuracy_vals = []\n",
    "test_loss_vals = []\n",
    "train_accuracy_vals = []\n",
    "train_loss_vals = []\n",
    "\n",
    "# Mélanger les données\n",
    "for i in range(num_splits) :\n",
    "    train_data_suffled = train_data\n",
    "    np.random.shuffle(train_data_suffled)\n",
    "    train_data_suffled[train_data_suffled[:, 0] == -1, 0] = 0\n",
    "    X_train, y_train = train_data_suffled[:, 1:], train_data_suffled[:, 0]\n",
    "    X_train_normalized = normalize(X_train, axis=1)\n",
    "\n",
    "    # build and compil model\n",
    "    input_shape = (96, 1)\n",
    "    input_layer = Input(input_shape)\n",
    "    conv_layer_1_1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(input_layer)\n",
    "    relu_layer_1_1 = Activation(hidden_activation)(conv_layer_1_1)\n",
    "    conv_layer_1_2 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(relu_layer_1_1)\n",
    "    relu_layer_1_2 = Activation(hidden_activation)(conv_layer_1_2)\n",
    "    pooling_layer_1 = MaxPooling1D(pool_size=pool_size, padding=padding)(relu_layer_1_2)\n",
    "    dropout_layer_1_1 = Dropout(rate=dropout_rate)(pooling_layer_1)\n",
    "    conv_layer_2_1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(dropout_layer_1_1)\n",
    "    relu_layer_2_1 = Activation(hidden_activation)(conv_layer_2_1)\n",
    "    conv_layer_2_2 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(relu_layer_2_1)\n",
    "    relu_layer_2_2 = Activation(hidden_activation)(conv_layer_2_2)\n",
    "    pooling_layer_2 = MaxPooling1D(pool_size=pool_size, padding=padding)(relu_layer_2_2)\n",
    "    flattened_layer = Flatten()(pooling_layer_2)\n",
    "    dropout_flattened = Dropout(rate=dropout_rate)(flattened_layer)\n",
    "    output_layer = Dense(units=nb_classes,activation=final_activation)(dropout_flattened)\n",
    "    model_cnn = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model_cnn.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model_checkpoint = callbacks.ModelCheckpoint('best_model_CNN.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    # start training\n",
    "    history = model_cnn.fit(X_train_normalized, y_train, \n",
    "                        batch_size=mini_batch_size, \n",
    "                        epochs=nb_epochs,\n",
    "                        validation_split=percentage_of_train_as_validation,\n",
    "                        verbose=False,\n",
    "                        callbacks=[model_checkpoint])\n",
    "\n",
    "    # Eveluate best model\n",
    "    best_model = models.load_model('best_model_CNN.keras')\n",
    "    train_loss, train_accuracy = best_model.evaluate(X_train_normalized, y_train)\n",
    "    test_loss, test_accuracy = best_model.evaluate(X_test_normalized, y_test)\n",
    "\n",
    "    test_accuracy_vals.append(test_accuracy)\n",
    "    test_loss_vals.append(test_loss)\n",
    "    train_accuracy_vals.append(train_accuracy)\n",
    "    train_loss_vals.append(train_loss)\n",
    "\n",
    "# Affichage des moyennes\n",
    "print(\"Moyenne de test_accuracy_vals:\", np.mean(test_accuracy_vals))\n",
    "print(\"Moyenne de test_loss_vals:\", np.mean(test_loss_vals))\n",
    "print(\"la variance associée de l'accuracy:\", np.var(test_accuracy_vals))\n",
    "print(\"la variance associée de la loss:\", np.var(test_loss_vals))\n",
    "\n",
    "print(\"\\nMoyenne de train_accuracy_vals:\", np.mean(train_accuracy_vals))\n",
    "print(\"Moyenne de train_loss_vals:\", np.mean(train_loss_vals))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
