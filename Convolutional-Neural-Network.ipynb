{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a6ffb2c272a0f5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Concolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8756f5c6a206c30e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Importation librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import normalize, plot_model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Activation, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras import models, callbacks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d38e41eb5da70",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data import and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850501202a077edc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data recovery\n",
    "train_data = np.loadtxt('data/ECG200_TRAIN.tsv', delimiter='\\t')\n",
    "test_data = np.loadtxt('data/ECG200_TEST.tsv', delimiter='\\t')\n",
    "\n",
    "# Show first time series\n",
    "print(\"health:\", train_data[0,0])\n",
    "print(train_data[0,1:])\n",
    "\n",
    "# Show data dimensions\n",
    "print(\"Dimensions des données d'entrainnement:\", train_data.shape)\n",
    "print(\"Dimensions des données de test:\", test_data.shape)\n",
    "\n",
    "# Show first 5 time series\n",
    "nb_series_a_afficher = 5\n",
    "plt.figure(figsize=(27, 7))\n",
    "for i in range(nb_series_a_afficher):\n",
    "    serie_temporelle = train_data[i, 1:]\n",
    "    etat_sante = train_data[i, 0]\n",
    "    \n",
    "    plage_temps = range(len(serie_temporelle))\n",
    "    \n",
    "    plt.subplot(2, nb_series_a_afficher, i+1)\n",
    "    plt.plot(plage_temps, serie_temporelle, label='Série temporelle')\n",
    "    plt.xlabel('Temps')\n",
    "    plt.ylabel('Valeurs')\n",
    "    plt.title(f'Série temporelle {i+1} (Santé: {\"Bon\" if etat_sante == -1 else \"Mauvais\"})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66daa99",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizer classes\n",
    "train_data[train_data[:, 0] == -1, 0] = 0\n",
    "test_data[test_data[:, 0] == -1, 0] = 0\n",
    "\n",
    "# Separation of training and testing data\n",
    "X_train, y_train = train_data[:, 1:], train_data[:, 0]\n",
    "X_test, y_test = test_data[:, 1:], test_data[:, 0]\n",
    "\n",
    "# Normalize time series between 0 and 1 independently of each other\n",
    "X_train = normalize(X_train, axis=1)\n",
    "X_test = normalize(X_test, axis=1)\n",
    "\n",
    "# Display first data\n",
    "print(\"health:\", y_train[0])\n",
    "print(X_train[0])\n",
    "print(\"Dimensions des données d'entrainnement:\", (X_train.shape, y_train.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52567e24",
   "metadata": {},
   "source": [
    "## Choice of Hyperparameters\n",
    "\n",
    "❒ Layer hyperparameters: kernel size, dropout, method of activation of hidden layers, method of activation of the final layer, etc.\n",
    "\n",
    "❒ The model compilation hyperparameters: optimizer, loss, learning rate, etc.\n",
    "\n",
    "❒ Model execution hyperparameters: batch size, number of epochs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers hyperparameters\n",
    "filters = 3\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "padding = 'same'\n",
    "hidden_activation = 'relu'\n",
    "final_activation = 'sigmoid' \n",
    "pool_size = 1\n",
    "dropout_rate = 0.3\n",
    "nb_classes = 1\n",
    "\n",
    "# Compil hyperparameters\n",
    "\"\"\"learning_rate = 0.001 # choose learning rate pour l'optimiseur adam\"\"\"                        \n",
    "optimizer_algo = 'adam'\n",
    "cost_function = 'binary_crossentropy'\n",
    "\n",
    "# Execution hyperparameters\n",
    "mini_batch_size = 16\n",
    "nb_epochs = 1500\n",
    "percentage_of_train_as_validation = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1df5f",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "input_shape = (96, 1)\n",
    "input_layer = Input(input_shape)\n",
    "\n",
    "# Hidden block\n",
    "conv_layer_1_1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding)(input_layer)\n",
    "relu_layer_1_1 = Activation(hidden_activation)(conv_layer_1_1)\n",
    "conv_layer_1_2 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding)(relu_layer_1_1)\n",
    "relu_layer_1_2 = Activation(hidden_activation)(conv_layer_1_2)\n",
    "pooling_layer_1 = MaxPooling1D(pool_size=pool_size,padding=padding)(relu_layer_1_2)\n",
    "dropout_layer_1_1 = Dropout(rate=dropout_rate)(relu_layer_1_1)\n",
    "\n",
    "# Output\n",
    "flattened_layer = Flatten()(pooling_layer_1)\n",
    "dropout_flattened = Dropout(rate=dropout_rate)(flattened_layer)\n",
    "output_layer = Dense(units=nb_classes, activation=final_activation)(dropout_flattened)\n",
    "\n",
    "model_cnn = models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfef551",
   "metadata": {},
   "source": [
    "## Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.compile(loss=cost_function,optimizer=optimizer_algo, metrics=['accuracy'])\n",
    "\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a38871",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model checkpoint (to save the best model for each epoch)\n",
    "model_checkpoint = callbacks.ModelCheckpoint('best_model_CNN.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Start training\n",
    "history = model_cnn.fit(X_train, y_train,\n",
    "                    batch_size=mini_batch_size, \n",
    "                    epochs=nb_epochs,\n",
    "                    validation_split=percentage_of_train_as_validation,\n",
    "                    verbose=False,\n",
    "                    callbacks=[model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a68424",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d58e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best validation's model\n",
    "best_model_cnn = models.load_model('best_model_CNN.keras')\n",
    "\n",
    "\n",
    "# Evaluate the model on train and test set\n",
    "train_loss, train_accuracy = best_model_cnn.evaluate(X_train, y_train)\n",
    "print(f\"Test Loss: {train_loss}\")\n",
    "print(f\"Test Accuracy: {train_accuracy}\")\n",
    "\n",
    "test_loss, test_accuracy = best_model_cnn.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e0af9",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fea15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465a802",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859856de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions about the test set\n",
    "y_pred = model_cnn.predict(X_test)\n",
    "\n",
    "# Convert y_pred probabilities into binary classes\n",
    "y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "class_names = ['Positif', 'Négatif']\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac8ea0",
   "metadata": {},
   "source": [
    "## Search for hyperparameters\n",
    "by grid search and random search (to do: with cross-validation)\n",
    "\n",
    "look hyperparameter_search_CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a187aed",
   "metadata": {},
   "source": [
    "## Best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f98c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametres\n",
    "filters = 5\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 'same'\n",
    "use_bias = False\n",
    "hidden_activation = 'relu'\n",
    "final_activation = 'sigmoid' \n",
    "pool_size = 2\n",
    "dropout_rate = 0.2\n",
    "nb_classes = 1\n",
    "optimizer_algo = 'adam'\n",
    "cost_function = 'binary_crossentropy'\n",
    "mini_batch_size = 16\n",
    "nb_epochs = 1000\n",
    "percentage_of_train_as_validation = 0.2\n",
    "\n",
    "# Build and compile model\n",
    "input_shape = (96, 1)\n",
    "input_layer = Input(input_shape)\n",
    "conv_layer_1_1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(input_layer)\n",
    "relu_layer_1_1 = Activation(hidden_activation)(conv_layer_1_1)\n",
    "conv_layer_1_2 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(relu_layer_1_1)\n",
    "relu_layer_1_2 = Activation(hidden_activation)(conv_layer_1_2)\n",
    "pooling_layer_1 = MaxPooling1D(pool_size=pool_size, padding=padding)(relu_layer_1_2)\n",
    "dropout_layer_1_1 = Dropout(rate=dropout_rate)(pooling_layer_1)\n",
    "conv_layer_2_1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(dropout_layer_1_1)\n",
    "relu_layer_2_1 = Activation(hidden_activation)(conv_layer_2_1)\n",
    "conv_layer_2_2 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(relu_layer_2_1)\n",
    "relu_layer_2_2 = Activation(hidden_activation)(conv_layer_2_2)\n",
    "pooling_layer_2 = MaxPooling1D(pool_size=pool_size, padding=padding)(relu_layer_2_2)\n",
    "flattened_layer = Flatten()(pooling_layer_2)\n",
    "dropout_flattened = Dropout(rate=dropout_rate)(flattened_layer)\n",
    "output_layer = Dense(units=nb_classes,activation=final_activation)(dropout_flattened)\n",
    "model_cnn = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "model_cnn.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint('best_model_CNN.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Start training\n",
    "start_training = time.time()\n",
    "history = model_cnn.fit(X_train, y_train, \n",
    "                    batch_size=mini_batch_size, \n",
    "                    epochs=nb_epochs,\n",
    "                    validation_split=percentage_of_train_as_validation,\n",
    "                    verbose=False,\n",
    "                    callbacks=[model_checkpoint])\n",
    "end_training = time.time()\n",
    "\n",
    "# Evaluate best model\n",
    "best_model_cnn = models.load_model('best_model_CNN.keras')\n",
    "train_loss, train_accuracy = best_model_cnn.evaluate(X_train, y_train)\n",
    "start_evaluate = time.time()\n",
    "test_loss, test_accuracy = best_model_cnn.evaluate(X_test, y_test)\n",
    "end_evaluate = time.time()\n",
    "\n",
    "y_pred = model_cnn.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "class_names = ['Positif', 'Négatif']\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Result\n",
    "model_cnn.summary()\n",
    "training_time_seconds = end_training - start_training\n",
    "evaluate_time_seconds = end_evaluate - start_evaluate\n",
    "print(f\"\\nNombre total de paramètres : {best_model_cnn.count_params()}\")\n",
    "print(f\"\\nTemps d'entraînement : {training_time_seconds:.3f} secondes.\")\n",
    "print(f\"Temps d'évaluation : {evaluate_time_seconds:.3f} secondes.\")\n",
    "print(f'\\nMoyenne de train_accuracy_vals: {np.mean(train_accuracy) * 100:.2f}%')\n",
    "print(f'Moyenne de train_loss_vals: {np.mean(train_loss) * 100:.2f}%')\n",
    "print(f'\\nMoyenne de test_accuracy_vals: {np.mean(test_accuracy) * 100:.2f}%')\n",
    "print(f'Moyenne de test_loss_vals: {np.mean(test_loss) * 100:.2f}%')\n",
    "print(f'\\nAUC-ROC : {roc_auc * 100:.2f}%')\n",
    "\n",
    "# Plot \n",
    "plt.figure(figsize=(16, 12))\n",
    "    # Training and Validation Accuracy\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "    # Training and Validation Loss\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "    # Confusion Matrix\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "    # ROC Curve\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0a399",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d13a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt('data/ECG200_TRAIN.tsv', delimiter='\\t')\n",
    "test_data = np.loadtxt('data/ECG200_TEST.tsv', delimiter='\\t')\n",
    "\n",
    "test_data[test_data[:, 0] == -1, 0] = 0\n",
    "X_test, y_test = test_data[:, 1:], test_data[:, 0]\n",
    "X_test = normalize(X_test, axis=1)\n",
    "\n",
    "# Hyperparameters\n",
    "filters = 5\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 'same'\n",
    "use_bias = False\n",
    "hidden_activation = 'relu'\n",
    "final_activation = 'sigmoid' \n",
    "pool_size = 2\n",
    "dropout_rate = 0.2\n",
    "nb_classes = 1\n",
    "optimizer_algo = 'adam'\n",
    "cost_function = 'binary_crossentropy'\n",
    "mini_batch_size = 16\n",
    "nb_epochs = 800\n",
    "percentage_of_train_as_validation = 0.2\n",
    "\n",
    "num_splits = 5\n",
    "test_accuracy_vals = []\n",
    "test_loss_vals = []\n",
    "train_accuracy_vals = []\n",
    "train_loss_vals = []\n",
    "\n",
    "for i in range(num_splits) :\n",
    "    train_data_suffled = train_data\n",
    "    np.random.shuffle(train_data_suffled)\n",
    "    train_data_suffled[train_data_suffled[:, 0] == -1, 0] = 0\n",
    "    X_train, y_train = train_data_suffled[:, 1:], train_data_suffled[:, 0]\n",
    "    X_trai = normalize(X_train, axis=1)\n",
    "\n",
    "    # build and compil model\n",
    "    input_shape = (96, 1)\n",
    "    input_layer = Input(input_shape)\n",
    "    conv_layer_1_1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(input_layer)\n",
    "    relu_layer_1_1 = Activation(hidden_activation)(conv_layer_1_1)\n",
    "    conv_layer_1_2 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(relu_layer_1_1)\n",
    "    relu_layer_1_2 = Activation(hidden_activation)(conv_layer_1_2)\n",
    "    pooling_layer_1 = MaxPooling1D(pool_size=pool_size, padding=padding)(relu_layer_1_2)\n",
    "    dropout_layer_1_1 = Dropout(rate=dropout_rate)(pooling_layer_1)\n",
    "    conv_layer_2_1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(dropout_layer_1_1)\n",
    "    relu_layer_2_1 = Activation(hidden_activation)(conv_layer_2_1)\n",
    "    conv_layer_2_2 = Conv1D(filters=filters,kernel_size=kernel_size,strides=stride,padding=padding,use_bias=use_bias)(relu_layer_2_1)\n",
    "    relu_layer_2_2 = Activation(hidden_activation)(conv_layer_2_2)\n",
    "    pooling_layer_2 = MaxPooling1D(pool_size=pool_size, padding=padding)(relu_layer_2_2)\n",
    "    flattened_layer = Flatten()(pooling_layer_2)\n",
    "    dropout_flattened = Dropout(rate=dropout_rate)(flattened_layer)\n",
    "    output_layer = Dense(units=nb_classes,activation=final_activation)(dropout_flattened)\n",
    "    model_cnn = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model_cnn.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model_checkpoint = callbacks.ModelCheckpoint('best_model_CNN.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    # start training\n",
    "    history = model_cnn.fit(X_train, y_train, \n",
    "                        batch_size=mini_batch_size, \n",
    "                        epochs=nb_epochs,\n",
    "                        validation_split=percentage_of_train_as_validation,\n",
    "                        verbose=False,\n",
    "                        callbacks=[model_checkpoint])\n",
    "\n",
    "    # evaluate best model\n",
    "    best_model_cnn = models.load_model('best_model_CNN.keras')\n",
    "    train_loss, train_accuracy = best_model_cnn.evaluate(X_train, y_train)\n",
    "    test_loss, test_accuracy = best_model_cnn.evaluate(X_test, y_test)\n",
    "\n",
    "    test_accuracy_vals.append(test_accuracy)\n",
    "    test_loss_vals.append(test_loss)\n",
    "    train_accuracy_vals.append(train_accuracy)\n",
    "    train_loss_vals.append(train_loss)\n",
    "\n",
    "# Display result\n",
    "print(\"\\nNombre d'essais :\", num_splits)\n",
    "print(f'\\nMoyenne de train_accuracy_vals: {np.mean(train_accuracy_vals) * 100:.2f}%')\n",
    "print(f'Moyenne de train_loss_vals: {np.mean(train_loss_vals) * 100:.2f}%')\n",
    "print(f'\\nMoyenne de test_accuracy_vals: {np.mean(test_accuracy_vals) * 100:.2f}%')\n",
    "print(f'Moyenne de test_loss_vals: {np.mean(test_loss_vals) * 100:.2f}%')\n",
    "print(f'\\tLa variance associée de l\\'accuracy: {np.var(test_accuracy_vals):.6f}')\n",
    "print(f'\\tLa variance associée de la loss: {np.var(test_loss_vals):.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
