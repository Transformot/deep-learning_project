{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-Layer Perceptron"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e619933b9e1de83c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6142b3b883c6c5cf"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-12T19:06:38.303370Z",
     "start_time": "2024-05-12T19:06:38.288901Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "outputs": [],
   "execution_count": 505
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d41f427f6ea3cf99"
  },
  {
   "cell_type": "code",
   "source": [
    "# Récupération des données en spécifiant le séparateur\n",
    "TRAIN_FILE_PATH = \"./data/ECG200_TRAIN.tsv\"\n",
    "TEST_FILE_PATH = \"./data/ECG200_TEST.tsv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILE_PATH, sep='\\t', header=None).dropna()\n",
    "test_df = pd.read_csv(TEST_FILE_PATH, sep='\\t', header=None).dropna()\n",
    "\n",
    "y_train = train_df.iloc[:, 0]\n",
    "X_train = train_df.iloc[:, 1:]\n",
    "\n",
    "y_test = test_df.iloc[:, 0]\n",
    "X_test = test_df.iloc[:, 1:]\n",
    "\n",
    "# Passage des classes -1/1 à 0/1\n",
    "y_train = y_train.replace(-1, 0)\n",
    "y_test = y_test.replace(-1, 0)\n",
    "\n",
    "# Normalisation des données\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "# Transformation en tableau NumPy\n",
    "X_train_array = np.asarray(X_train_normalized)\n",
    "X_test_array = np.asarray(X_test_normalized)\n",
    "\n",
    "y_train_array = np.asarray(y_train)\n",
    "y_test_array = np.asarray(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T19:06:38.318854Z",
     "start_time": "2024-05-12T19:06:38.304875Z"
    }
   },
   "id": "bb62b622add25e10",
   "outputs": [],
   "execution_count": 506
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T19:06:38.411490Z",
     "start_time": "2024-05-12T19:06:38.319855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Construction du MLP\n",
    "input_shape = X_train_array.shape[1:]\n",
    "input_layer = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "hidden_layer_1 = tf.keras.layers.Dense(units=32, activation='relu')(input_layer)\n",
    "dropout_1 = tf.keras.layers.Dropout(rate=0.2)(hidden_layer_1)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(dropout_1)\n",
    "\n",
    "# Création du modèle\n",
    "model_mlp = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model_mlp.summary()\n",
    "tf.keras.utils.plot_model(model_mlp, show_shapes=True)"
   ],
   "id": "44b00ab53c4064bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_72\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_73 (InputLayer)       [(None, 96)]              0         \n",
      "                                                                 \n",
      " dense_199 (Dense)           (None, 32)                3104      \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_200 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,137\n",
      "Trainable params: 3,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 507
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T19:06:38.426992Z",
     "start_time": "2024-05-12T19:06:38.412488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compilation du modèle\n",
    "learning_rate = 0.001\n",
    "optimizer_algo = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "cost_function = tf.keras.losses.binary_crossentropy\n",
    "\n",
    "model_mlp.compile(loss=cost_function, optimizer=optimizer_algo, metrics=['accuracy'])"
   ],
   "id": "33acd007ad03ed18",
   "outputs": [],
   "execution_count": 508
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T19:06:45.407026Z",
     "start_time": "2024-05-12T19:06:38.428010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Entraînement du modèle\n",
    "nb_epochs = 300\n",
    "mini_batch_size = 32\n",
    "percentage_of_train_as_validation = 0.2\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best-model.h5', monitor='val_loss', mode='min',\n",
    "                                                      save_best_only=True, verbose=1)\n",
    "\n",
    "history_mlp = model_mlp.fit(X_train_array, y_train_array,\n",
    "                            batch_size=mini_batch_size,\n",
    "                            epochs=nb_epochs,\n",
    "                            verbose=True,\n",
    "                            validation_split=percentage_of_train_as_validation,\n",
    "                            callbacks=[model_checkpoint])"
   ],
   "id": "1b576194bfb1e15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.5308 - accuracy: 0.7812\n",
      "Epoch 1: val_loss improved from inf to 0.70963, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.5708 - accuracy: 0.7250 - val_loss: 0.7096 - val_accuracy: 0.5500\n",
      "Epoch 2/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.5907 - accuracy: 0.6562\n",
      "Epoch 2: val_loss improved from 0.70963 to 0.69369, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5304 - accuracy: 0.7250 - val_loss: 0.6937 - val_accuracy: 0.5500\n",
      "Epoch 3/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.5392 - accuracy: 0.7500\n",
      "Epoch 3: val_loss improved from 0.69369 to 0.67535, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4972 - accuracy: 0.7375 - val_loss: 0.6754 - val_accuracy: 0.5500\n",
      "Epoch 4/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.5934 - accuracy: 0.6562\n",
      "Epoch 4: val_loss improved from 0.67535 to 0.64955, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4762 - accuracy: 0.7375 - val_loss: 0.6496 - val_accuracy: 0.6000\n",
      "Epoch 5/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3947 - accuracy: 0.8438\n",
      "Epoch 5: val_loss improved from 0.64955 to 0.63010, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4925 - accuracy: 0.7250 - val_loss: 0.6301 - val_accuracy: 0.6000\n",
      "Epoch 6/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4821 - accuracy: 0.8125\n",
      "Epoch 6: val_loss improved from 0.63010 to 0.60386, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4390 - accuracy: 0.8125 - val_loss: 0.6039 - val_accuracy: 0.6500\n",
      "Epoch 7/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4887 - accuracy: 0.7188\n",
      "Epoch 7: val_loss improved from 0.60386 to 0.59117, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4400 - accuracy: 0.7875 - val_loss: 0.5912 - val_accuracy: 0.6500\n",
      "Epoch 8/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4471 - accuracy: 0.7188\n",
      "Epoch 8: val_loss improved from 0.59117 to 0.58662, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4336 - accuracy: 0.7625 - val_loss: 0.5866 - val_accuracy: 0.6500\n",
      "Epoch 9/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3716 - accuracy: 0.8438\n",
      "Epoch 9: val_loss did not improve from 0.58662\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4389 - accuracy: 0.7750 - val_loss: 0.5906 - val_accuracy: 0.6500\n",
      "Epoch 10/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3432 - accuracy: 0.8750\n",
      "Epoch 10: val_loss did not improve from 0.58662\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4038 - accuracy: 0.7875 - val_loss: 0.5890 - val_accuracy: 0.6500\n",
      "Epoch 11/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3207 - accuracy: 0.8438\n",
      "Epoch 11: val_loss improved from 0.58662 to 0.58637, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4147 - accuracy: 0.7625 - val_loss: 0.5864 - val_accuracy: 0.6500\n",
      "Epoch 12/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3861 - accuracy: 0.7812\n",
      "Epoch 12: val_loss improved from 0.58637 to 0.58021, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3916 - accuracy: 0.8000 - val_loss: 0.5802 - val_accuracy: 0.6500\n",
      "Epoch 13/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4145 - accuracy: 0.8125\n",
      "Epoch 13: val_loss improved from 0.58021 to 0.57768, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.3919 - accuracy: 0.8250 - val_loss: 0.5777 - val_accuracy: 0.6500\n",
      "Epoch 14/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2872 - accuracy: 0.8750\n",
      "Epoch 14: val_loss did not improve from 0.57768\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3682 - accuracy: 0.8375 - val_loss: 0.5807 - val_accuracy: 0.6500\n",
      "Epoch 15/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3513 - accuracy: 0.8438\n",
      "Epoch 15: val_loss improved from 0.57768 to 0.57612, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3830 - accuracy: 0.8125 - val_loss: 0.5761 - val_accuracy: 0.7000\n",
      "Epoch 16/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3274 - accuracy: 0.8438\n",
      "Epoch 16: val_loss improved from 0.57612 to 0.57047, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3741 - accuracy: 0.8375 - val_loss: 0.5705 - val_accuracy: 0.7000\n",
      "Epoch 17/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3567 - accuracy: 0.8438\n",
      "Epoch 17: val_loss improved from 0.57047 to 0.56428, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3611 - accuracy: 0.8500 - val_loss: 0.5643 - val_accuracy: 0.7000\n",
      "Epoch 18/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2960 - accuracy: 0.9062\n",
      "Epoch 18: val_loss improved from 0.56428 to 0.55887, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3580 - accuracy: 0.8000 - val_loss: 0.5589 - val_accuracy: 0.7000\n",
      "Epoch 19/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3966 - accuracy: 0.7812\n",
      "Epoch 19: val_loss improved from 0.55887 to 0.55480, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3503 - accuracy: 0.8500 - val_loss: 0.5548 - val_accuracy: 0.7000\n",
      "Epoch 20/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3856 - accuracy: 0.8125\n",
      "Epoch 20: val_loss improved from 0.55480 to 0.55191, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3713 - accuracy: 0.7750 - val_loss: 0.5519 - val_accuracy: 0.7000\n",
      "Epoch 21/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3210 - accuracy: 0.8750\n",
      "Epoch 21: val_loss improved from 0.55191 to 0.55115, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3450 - accuracy: 0.8625 - val_loss: 0.5511 - val_accuracy: 0.7000\n",
      "Epoch 22/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3304 - accuracy: 0.8750\n",
      "Epoch 22: val_loss did not improve from 0.55115\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3568 - accuracy: 0.8500 - val_loss: 0.5542 - val_accuracy: 0.7000\n",
      "Epoch 23/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2804 - accuracy: 0.8750\n",
      "Epoch 23: val_loss did not improve from 0.55115\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3548 - accuracy: 0.8000 - val_loss: 0.5571 - val_accuracy: 0.7000\n",
      "Epoch 24/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3558 - accuracy: 0.8438\n",
      "Epoch 24: val_loss improved from 0.55115 to 0.54417, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3307 - accuracy: 0.8375 - val_loss: 0.5442 - val_accuracy: 0.7000\n",
      "Epoch 25/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3002 - accuracy: 0.8438\n",
      "Epoch 25: val_loss improved from 0.54417 to 0.53635, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3235 - accuracy: 0.8375 - val_loss: 0.5364 - val_accuracy: 0.7000\n",
      "Epoch 26/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2221 - accuracy: 0.9375\n",
      "Epoch 26: val_loss improved from 0.53635 to 0.52918, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3278 - accuracy: 0.8750 - val_loss: 0.5292 - val_accuracy: 0.7000\n",
      "Epoch 27/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3770 - accuracy: 0.8125\n",
      "Epoch 27: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3396 - accuracy: 0.7875 - val_loss: 0.5294 - val_accuracy: 0.7000\n",
      "Epoch 28/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2999 - accuracy: 0.8438\n",
      "Epoch 28: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2887 - accuracy: 0.8750 - val_loss: 0.5308 - val_accuracy: 0.7000\n",
      "Epoch 29/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3293 - accuracy: 0.8438\n",
      "Epoch 29: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3152 - accuracy: 0.8375 - val_loss: 0.5316 - val_accuracy: 0.7500\n",
      "Epoch 30/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2977 - accuracy: 0.8125\n",
      "Epoch 30: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3066 - accuracy: 0.8500 - val_loss: 0.5420 - val_accuracy: 0.7000\n",
      "Epoch 31/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3366 - accuracy: 0.8438\n",
      "Epoch 31: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2929 - accuracy: 0.8750 - val_loss: 0.5533 - val_accuracy: 0.7000\n",
      "Epoch 32/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2923 - accuracy: 0.9062\n",
      "Epoch 32: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3194 - accuracy: 0.8625 - val_loss: 0.5604 - val_accuracy: 0.7000\n",
      "Epoch 33/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2260 - accuracy: 0.8750\n",
      "Epoch 33: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3167 - accuracy: 0.8375 - val_loss: 0.5586 - val_accuracy: 0.7000\n",
      "Epoch 34/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2511 - accuracy: 0.9062\n",
      "Epoch 34: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3083 - accuracy: 0.8625 - val_loss: 0.5575 - val_accuracy: 0.7000\n",
      "Epoch 35/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2329 - accuracy: 0.8750\n",
      "Epoch 35: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3015 - accuracy: 0.8375 - val_loss: 0.5516 - val_accuracy: 0.7500\n",
      "Epoch 36/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1825 - accuracy: 0.9688\n",
      "Epoch 36: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2976 - accuracy: 0.8750 - val_loss: 0.5380 - val_accuracy: 0.7500\n",
      "Epoch 37/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2803 - accuracy: 0.8750\n",
      "Epoch 37: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3123 - accuracy: 0.8500 - val_loss: 0.5309 - val_accuracy: 0.7500\n",
      "Epoch 38/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3271 - accuracy: 0.8438\n",
      "Epoch 38: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2877 - accuracy: 0.8625 - val_loss: 0.5298 - val_accuracy: 0.7000\n",
      "Epoch 39/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2304 - accuracy: 0.8750\n",
      "Epoch 39: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3041 - accuracy: 0.8375 - val_loss: 0.5376 - val_accuracy: 0.7500\n",
      "Epoch 40/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2934 - accuracy: 0.9062\n",
      "Epoch 40: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2875 - accuracy: 0.8625 - val_loss: 0.5480 - val_accuracy: 0.7500\n",
      "Epoch 41/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2587 - accuracy: 0.9062\n",
      "Epoch 41: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2867 - accuracy: 0.8750 - val_loss: 0.5606 - val_accuracy: 0.7500\n",
      "Epoch 42/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4140 - accuracy: 0.7812\n",
      "Epoch 42: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2896 - accuracy: 0.8750 - val_loss: 0.5545 - val_accuracy: 0.7500\n",
      "Epoch 43/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3122 - accuracy: 0.8125\n",
      "Epoch 43: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2775 - accuracy: 0.8500 - val_loss: 0.5490 - val_accuracy: 0.7000\n",
      "Epoch 44/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3176 - accuracy: 0.8438\n",
      "Epoch 44: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2879 - accuracy: 0.8625 - val_loss: 0.5434 - val_accuracy: 0.7000\n",
      "Epoch 45/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2355 - accuracy: 0.9375\n",
      "Epoch 45: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2910 - accuracy: 0.8625 - val_loss: 0.5432 - val_accuracy: 0.7000\n",
      "Epoch 46/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2882 - accuracy: 0.8750\n",
      "Epoch 46: val_loss did not improve from 0.52918\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2793 - accuracy: 0.8875 - val_loss: 0.5355 - val_accuracy: 0.7000\n",
      "Epoch 47/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2920 - accuracy: 0.8438\n",
      "Epoch 47: val_loss improved from 0.52918 to 0.52832, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.2657 - accuracy: 0.8625 - val_loss: 0.5283 - val_accuracy: 0.7000\n",
      "Epoch 48/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3060 - accuracy: 0.8438\n",
      "Epoch 48: val_loss improved from 0.52832 to 0.52375, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3020 - accuracy: 0.8625 - val_loss: 0.5238 - val_accuracy: 0.7000\n",
      "Epoch 49/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2815 - accuracy: 0.9062\n",
      "Epoch 49: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2825 - accuracy: 0.9000 - val_loss: 0.5262 - val_accuracy: 0.7000\n",
      "Epoch 50/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2744 - accuracy: 0.8750\n",
      "Epoch 50: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2872 - accuracy: 0.8500 - val_loss: 0.5405 - val_accuracy: 0.7000\n",
      "Epoch 51/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2261 - accuracy: 0.9375\n",
      "Epoch 51: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2994 - accuracy: 0.8625 - val_loss: 0.5542 - val_accuracy: 0.7000\n",
      "Epoch 52/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2577 - accuracy: 0.8750\n",
      "Epoch 52: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2524 - accuracy: 0.8625 - val_loss: 0.5539 - val_accuracy: 0.7000\n",
      "Epoch 53/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2615 - accuracy: 0.9375\n",
      "Epoch 53: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2781 - accuracy: 0.8875 - val_loss: 0.5514 - val_accuracy: 0.7000\n",
      "Epoch 54/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1714 - accuracy: 0.9688\n",
      "Epoch 54: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2475 - accuracy: 0.8875 - val_loss: 0.5547 - val_accuracy: 0.7000\n",
      "Epoch 55/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2425 - accuracy: 0.8438\n",
      "Epoch 55: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2684 - accuracy: 0.8750 - val_loss: 0.5529 - val_accuracy: 0.7000\n",
      "Epoch 56/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2638 - accuracy: 0.9062\n",
      "Epoch 56: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2350 - accuracy: 0.9125 - val_loss: 0.5444 - val_accuracy: 0.7000\n",
      "Epoch 57/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2100 - accuracy: 0.9062\n",
      "Epoch 57: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2759 - accuracy: 0.8625 - val_loss: 0.5381 - val_accuracy: 0.7000\n",
      "Epoch 58/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3126 - accuracy: 0.8125\n",
      "Epoch 58: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2590 - accuracy: 0.8500 - val_loss: 0.5389 - val_accuracy: 0.7000\n",
      "Epoch 59/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2835 - accuracy: 0.8438\n",
      "Epoch 59: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2578 - accuracy: 0.8625 - val_loss: 0.5434 - val_accuracy: 0.7000\n",
      "Epoch 60/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3127 - accuracy: 0.8438\n",
      "Epoch 60: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2836 - accuracy: 0.8750 - val_loss: 0.5568 - val_accuracy: 0.7000\n",
      "Epoch 61/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2648 - accuracy: 0.8750\n",
      "Epoch 61: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2532 - accuracy: 0.9000 - val_loss: 0.5726 - val_accuracy: 0.7000\n",
      "Epoch 62/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1296 - accuracy: 0.9688\n",
      "Epoch 62: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2436 - accuracy: 0.8875 - val_loss: 0.5763 - val_accuracy: 0.7000\n",
      "Epoch 63/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2105 - accuracy: 0.9375\n",
      "Epoch 63: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2832 - accuracy: 0.8875 - val_loss: 0.5735 - val_accuracy: 0.7000\n",
      "Epoch 64/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2807 - accuracy: 0.8750\n",
      "Epoch 64: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2315 - accuracy: 0.9250 - val_loss: 0.5599 - val_accuracy: 0.7000\n",
      "Epoch 65/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2909 - accuracy: 0.8125\n",
      "Epoch 65: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2600 - accuracy: 0.8500 - val_loss: 0.5513 - val_accuracy: 0.7000\n",
      "Epoch 66/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1981 - accuracy: 0.9375\n",
      "Epoch 66: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2776 - accuracy: 0.8625 - val_loss: 0.5477 - val_accuracy: 0.7000\n",
      "Epoch 67/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2198 - accuracy: 0.9375\n",
      "Epoch 67: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2550 - accuracy: 0.9250 - val_loss: 0.5490 - val_accuracy: 0.7000\n",
      "Epoch 68/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2110 - accuracy: 0.8750\n",
      "Epoch 68: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2452 - accuracy: 0.8875 - val_loss: 0.5585 - val_accuracy: 0.7000\n",
      "Epoch 69/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2353 - accuracy: 0.9062\n",
      "Epoch 69: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2559 - accuracy: 0.8750 - val_loss: 0.5590 - val_accuracy: 0.7000\n",
      "Epoch 70/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2959 - accuracy: 0.8438\n",
      "Epoch 70: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2315 - accuracy: 0.9000 - val_loss: 0.5519 - val_accuracy: 0.7000\n",
      "Epoch 71/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2983 - accuracy: 0.8750\n",
      "Epoch 71: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2296 - accuracy: 0.8750 - val_loss: 0.5506 - val_accuracy: 0.7000\n",
      "Epoch 72/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3105 - accuracy: 0.8125\n",
      "Epoch 72: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2309 - accuracy: 0.8875 - val_loss: 0.5577 - val_accuracy: 0.7000\n",
      "Epoch 73/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2515 - accuracy: 0.9062\n",
      "Epoch 73: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2179 - accuracy: 0.9125 - val_loss: 0.5674 - val_accuracy: 0.7000\n",
      "Epoch 74/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3599 - accuracy: 0.8125\n",
      "Epoch 74: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2417 - accuracy: 0.9000 - val_loss: 0.5698 - val_accuracy: 0.7000\n",
      "Epoch 75/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2323 - accuracy: 0.8750\n",
      "Epoch 75: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2320 - accuracy: 0.9000 - val_loss: 0.5644 - val_accuracy: 0.7000\n",
      "Epoch 76/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2277 - accuracy: 0.9062\n",
      "Epoch 76: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2340 - accuracy: 0.9000 - val_loss: 0.5649 - val_accuracy: 0.7000\n",
      "Epoch 77/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3221 - accuracy: 0.8438\n",
      "Epoch 77: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2476 - accuracy: 0.9000 - val_loss: 0.5604 - val_accuracy: 0.7000\n",
      "Epoch 78/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2462 - accuracy: 0.9375\n",
      "Epoch 78: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2323 - accuracy: 0.8875 - val_loss: 0.5558 - val_accuracy: 0.7000\n",
      "Epoch 79/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1711 - accuracy: 0.9688\n",
      "Epoch 79: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2441 - accuracy: 0.9000 - val_loss: 0.5508 - val_accuracy: 0.7000\n",
      "Epoch 80/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1137 - accuracy: 1.0000\n",
      "Epoch 80: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2492 - accuracy: 0.9125 - val_loss: 0.5465 - val_accuracy: 0.7000\n",
      "Epoch 81/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2373 - accuracy: 0.8438\n",
      "Epoch 81: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2512 - accuracy: 0.8500 - val_loss: 0.5483 - val_accuracy: 0.7000\n",
      "Epoch 82/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2553 - accuracy: 0.9062\n",
      "Epoch 82: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2596 - accuracy: 0.8750 - val_loss: 0.5489 - val_accuracy: 0.7000\n",
      "Epoch 83/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2770 - accuracy: 0.8750\n",
      "Epoch 83: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2528 - accuracy: 0.8875 - val_loss: 0.5584 - val_accuracy: 0.7000\n",
      "Epoch 84/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2783 - accuracy: 0.8750\n",
      "Epoch 84: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2287 - accuracy: 0.9125 - val_loss: 0.5662 - val_accuracy: 0.7000\n",
      "Epoch 85/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2000 - accuracy: 0.9062\n",
      "Epoch 85: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2437 - accuracy: 0.8875 - val_loss: 0.5691 - val_accuracy: 0.7000\n",
      "Epoch 86/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2455 - accuracy: 0.8438\n",
      "Epoch 86: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2342 - accuracy: 0.8750 - val_loss: 0.5667 - val_accuracy: 0.7000\n",
      "Epoch 87/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2569 - accuracy: 0.8750\n",
      "Epoch 87: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2607 - accuracy: 0.8500 - val_loss: 0.5517 - val_accuracy: 0.7000\n",
      "Epoch 88/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2894 - accuracy: 0.8438\n",
      "Epoch 88: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2410 - accuracy: 0.8750 - val_loss: 0.5421 - val_accuracy: 0.7000\n",
      "Epoch 89/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1990 - accuracy: 0.8750\n",
      "Epoch 89: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2236 - accuracy: 0.8875 - val_loss: 0.5450 - val_accuracy: 0.7000\n",
      "Epoch 90/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2745 - accuracy: 0.8125\n",
      "Epoch 90: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2459 - accuracy: 0.8625 - val_loss: 0.5542 - val_accuracy: 0.7000\n",
      "Epoch 91/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2432 - accuracy: 0.8750\n",
      "Epoch 91: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2218 - accuracy: 0.9125 - val_loss: 0.5601 - val_accuracy: 0.7000\n",
      "Epoch 92/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2262 - accuracy: 0.8750\n",
      "Epoch 92: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2515 - accuracy: 0.9000 - val_loss: 0.5659 - val_accuracy: 0.7000\n",
      "Epoch 93/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2343 - accuracy: 0.8438\n",
      "Epoch 93: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2310 - accuracy: 0.9000 - val_loss: 0.5610 - val_accuracy: 0.7000\n",
      "Epoch 94/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2282 - accuracy: 0.8750\n",
      "Epoch 94: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2148 - accuracy: 0.9000 - val_loss: 0.5495 - val_accuracy: 0.7000\n",
      "Epoch 95/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2736 - accuracy: 0.8750\n",
      "Epoch 95: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2553 - accuracy: 0.8875 - val_loss: 0.5369 - val_accuracy: 0.7000\n",
      "Epoch 96/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1467 - accuracy: 0.9375\n",
      "Epoch 96: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2341 - accuracy: 0.8875 - val_loss: 0.5295 - val_accuracy: 0.7000\n",
      "Epoch 97/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1578 - accuracy: 0.9375\n",
      "Epoch 97: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2554 - accuracy: 0.8875 - val_loss: 0.5280 - val_accuracy: 0.7000\n",
      "Epoch 98/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2915 - accuracy: 0.8125\n",
      "Epoch 98: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2138 - accuracy: 0.8875 - val_loss: 0.5388 - val_accuracy: 0.7000\n",
      "Epoch 99/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2723 - accuracy: 0.8750\n",
      "Epoch 99: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2073 - accuracy: 0.9000 - val_loss: 0.5544 - val_accuracy: 0.7000\n",
      "Epoch 100/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2234 - accuracy: 0.9375\n",
      "Epoch 100: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2311 - accuracy: 0.9000 - val_loss: 0.5725 - val_accuracy: 0.7000\n",
      "Epoch 101/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2234 - accuracy: 0.8750\n",
      "Epoch 101: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2267 - accuracy: 0.9000 - val_loss: 0.5833 - val_accuracy: 0.7000\n",
      "Epoch 102/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1331 - accuracy: 1.0000\n",
      "Epoch 102: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2022 - accuracy: 0.9500 - val_loss: 0.5809 - val_accuracy: 0.7000\n",
      "Epoch 103/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0790 - accuracy: 0.9688\n",
      "Epoch 103: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2040 - accuracy: 0.9125 - val_loss: 0.5647 - val_accuracy: 0.7000\n",
      "Epoch 104/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2964 - accuracy: 0.8438\n",
      "Epoch 104: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2096 - accuracy: 0.9000 - val_loss: 0.5443 - val_accuracy: 0.7000\n",
      "Epoch 105/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2118 - accuracy: 0.9062\n",
      "Epoch 105: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2193 - accuracy: 0.9000 - val_loss: 0.5381 - val_accuracy: 0.7000\n",
      "Epoch 106/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2072 - accuracy: 0.8750\n",
      "Epoch 106: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1875 - accuracy: 0.9125 - val_loss: 0.5437 - val_accuracy: 0.7000\n",
      "Epoch 107/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1481 - accuracy: 0.9688\n",
      "Epoch 107: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2122 - accuracy: 0.9125 - val_loss: 0.5528 - val_accuracy: 0.7000\n",
      "Epoch 108/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2272 - accuracy: 0.8750\n",
      "Epoch 108: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2120 - accuracy: 0.9250 - val_loss: 0.5549 - val_accuracy: 0.7000\n",
      "Epoch 109/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2936 - accuracy: 0.9062\n",
      "Epoch 109: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2146 - accuracy: 0.9125 - val_loss: 0.5599 - val_accuracy: 0.7000\n",
      "Epoch 110/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2914 - accuracy: 0.9062\n",
      "Epoch 110: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2086 - accuracy: 0.9125 - val_loss: 0.5726 - val_accuracy: 0.7000\n",
      "Epoch 111/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2648 - accuracy: 0.8750\n",
      "Epoch 111: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2055 - accuracy: 0.9125 - val_loss: 0.5768 - val_accuracy: 0.7000\n",
      "Epoch 112/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2327 - accuracy: 0.8750\n",
      "Epoch 112: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2441 - accuracy: 0.8625 - val_loss: 0.5729 - val_accuracy: 0.7000\n",
      "Epoch 113/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1941 - accuracy: 0.8750\n",
      "Epoch 113: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2196 - accuracy: 0.9000 - val_loss: 0.5556 - val_accuracy: 0.7000\n",
      "Epoch 114/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1735 - accuracy: 0.8750\n",
      "Epoch 114: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2328 - accuracy: 0.8875 - val_loss: 0.5383 - val_accuracy: 0.7000\n",
      "Epoch 115/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1857 - accuracy: 0.9062\n",
      "Epoch 115: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1931 - accuracy: 0.9125 - val_loss: 0.5351 - val_accuracy: 0.7000\n",
      "Epoch 116/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3033 - accuracy: 0.8750\n",
      "Epoch 116: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2253 - accuracy: 0.8875 - val_loss: 0.5398 - val_accuracy: 0.7000\n",
      "Epoch 117/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3058 - accuracy: 0.7812\n",
      "Epoch 117: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2154 - accuracy: 0.8625 - val_loss: 0.5448 - val_accuracy: 0.7000\n",
      "Epoch 118/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2832 - accuracy: 0.8438\n",
      "Epoch 118: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2094 - accuracy: 0.9000 - val_loss: 0.5479 - val_accuracy: 0.7000\n",
      "Epoch 119/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2220 - accuracy: 0.9062\n",
      "Epoch 119: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2021 - accuracy: 0.9125 - val_loss: 0.5624 - val_accuracy: 0.7000\n",
      "Epoch 120/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2383 - accuracy: 0.8750\n",
      "Epoch 120: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1939 - accuracy: 0.9125 - val_loss: 0.5768 - val_accuracy: 0.7000\n",
      "Epoch 121/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2106 - accuracy: 0.8750\n",
      "Epoch 121: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1951 - accuracy: 0.9250 - val_loss: 0.5744 - val_accuracy: 0.7000\n",
      "Epoch 122/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1382 - accuracy: 0.9375\n",
      "Epoch 122: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1851 - accuracy: 0.9125 - val_loss: 0.5738 - val_accuracy: 0.7000\n",
      "Epoch 123/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1488 - accuracy: 0.9375\n",
      "Epoch 123: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1905 - accuracy: 0.9125 - val_loss: 0.5723 - val_accuracy: 0.7000\n",
      "Epoch 124/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2004 - accuracy: 0.8750\n",
      "Epoch 124: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1921 - accuracy: 0.9125 - val_loss: 0.5547 - val_accuracy: 0.7000\n",
      "Epoch 125/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2534 - accuracy: 0.8750\n",
      "Epoch 125: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1869 - accuracy: 0.9250 - val_loss: 0.5397 - val_accuracy: 0.7000\n",
      "Epoch 126/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3128 - accuracy: 0.8438\n",
      "Epoch 126: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2304 - accuracy: 0.9000 - val_loss: 0.5378 - val_accuracy: 0.7000\n",
      "Epoch 127/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2426 - accuracy: 0.9062\n",
      "Epoch 127: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2088 - accuracy: 0.9000 - val_loss: 0.5394 - val_accuracy: 0.7000\n",
      "Epoch 128/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2520 - accuracy: 0.8750\n",
      "Epoch 128: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2142 - accuracy: 0.9250 - val_loss: 0.5410 - val_accuracy: 0.7000\n",
      "Epoch 129/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1648 - accuracy: 0.9062\n",
      "Epoch 129: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1967 - accuracy: 0.9000 - val_loss: 0.5449 - val_accuracy: 0.7000\n",
      "Epoch 130/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1670 - accuracy: 0.9375\n",
      "Epoch 130: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1963 - accuracy: 0.9000 - val_loss: 0.5489 - val_accuracy: 0.7000\n",
      "Epoch 131/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2010 - accuracy: 0.8750\n",
      "Epoch 131: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2042 - accuracy: 0.9000 - val_loss: 0.5549 - val_accuracy: 0.7000\n",
      "Epoch 132/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2099 - accuracy: 0.9375\n",
      "Epoch 132: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1956 - accuracy: 0.9125 - val_loss: 0.5642 - val_accuracy: 0.7000\n",
      "Epoch 133/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1756 - accuracy: 0.9375\n",
      "Epoch 133: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1828 - accuracy: 0.9250 - val_loss: 0.5667 - val_accuracy: 0.7000\n",
      "Epoch 134/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2580 - accuracy: 0.8750\n",
      "Epoch 134: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2122 - accuracy: 0.9125 - val_loss: 0.5735 - val_accuracy: 0.7000\n",
      "Epoch 135/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1684 - accuracy: 0.9062\n",
      "Epoch 135: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1985 - accuracy: 0.9000 - val_loss: 0.5583 - val_accuracy: 0.7000\n",
      "Epoch 136/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1874 - accuracy: 0.9375\n",
      "Epoch 136: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2100 - accuracy: 0.9000 - val_loss: 0.5496 - val_accuracy: 0.7000\n",
      "Epoch 137/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1388 - accuracy: 0.9375\n",
      "Epoch 137: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1913 - accuracy: 0.9250 - val_loss: 0.5462 - val_accuracy: 0.7000\n",
      "Epoch 138/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1457 - accuracy: 0.9688\n",
      "Epoch 138: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1916 - accuracy: 0.9375 - val_loss: 0.5472 - val_accuracy: 0.7000\n",
      "Epoch 139/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1128 - accuracy: 0.9375\n",
      "Epoch 139: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1972 - accuracy: 0.9250 - val_loss: 0.5442 - val_accuracy: 0.7000\n",
      "Epoch 140/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1130 - accuracy: 0.9375\n",
      "Epoch 140: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2083 - accuracy: 0.9000 - val_loss: 0.5379 - val_accuracy: 0.7000\n",
      "Epoch 141/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1466 - accuracy: 0.9375\n",
      "Epoch 141: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2169 - accuracy: 0.9125 - val_loss: 0.5372 - val_accuracy: 0.7000\n",
      "Epoch 142/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2064 - accuracy: 0.9375\n",
      "Epoch 142: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1994 - accuracy: 0.9250 - val_loss: 0.5393 - val_accuracy: 0.7000\n",
      "Epoch 143/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1948 - accuracy: 0.9375\n",
      "Epoch 143: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1718 - accuracy: 0.9375 - val_loss: 0.5513 - val_accuracy: 0.7000\n",
      "Epoch 144/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2006 - accuracy: 0.9688\n",
      "Epoch 144: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1763 - accuracy: 0.9625 - val_loss: 0.5510 - val_accuracy: 0.7000\n",
      "Epoch 145/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1745 - accuracy: 0.9688\n",
      "Epoch 145: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1654 - accuracy: 0.9375 - val_loss: 0.5516 - val_accuracy: 0.7000\n",
      "Epoch 146/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1950 - accuracy: 0.9375\n",
      "Epoch 146: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1995 - accuracy: 0.9125 - val_loss: 0.5551 - val_accuracy: 0.7000\n",
      "Epoch 147/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1958 - accuracy: 0.9062\n",
      "Epoch 147: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1946 - accuracy: 0.9250 - val_loss: 0.5541 - val_accuracy: 0.7000\n",
      "Epoch 148/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1572 - accuracy: 0.9375\n",
      "Epoch 148: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1924 - accuracy: 0.9125 - val_loss: 0.5447 - val_accuracy: 0.7000\n",
      "Epoch 149/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2150 - accuracy: 0.9375\n",
      "Epoch 149: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2063 - accuracy: 0.9375 - val_loss: 0.5354 - val_accuracy: 0.7000\n",
      "Epoch 150/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1455 - accuracy: 0.9688\n",
      "Epoch 150: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1735 - accuracy: 0.9375 - val_loss: 0.5299 - val_accuracy: 0.7000\n",
      "Epoch 151/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2073 - accuracy: 0.9375\n",
      "Epoch 151: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1927 - accuracy: 0.9000 - val_loss: 0.5366 - val_accuracy: 0.7000\n",
      "Epoch 152/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2772 - accuracy: 0.9062\n",
      "Epoch 152: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1840 - accuracy: 0.9500 - val_loss: 0.5444 - val_accuracy: 0.7000\n",
      "Epoch 153/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0613 - accuracy: 1.0000\n",
      "Epoch 153: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1805 - accuracy: 0.9250 - val_loss: 0.5476 - val_accuracy: 0.7000\n",
      "Epoch 154/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2045 - accuracy: 0.9375\n",
      "Epoch 154: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2001 - accuracy: 0.9250 - val_loss: 0.5432 - val_accuracy: 0.7000\n",
      "Epoch 155/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2758 - accuracy: 0.8750\n",
      "Epoch 155: val_loss did not improve from 0.52375\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1917 - accuracy: 0.9125 - val_loss: 0.5270 - val_accuracy: 0.7000\n",
      "Epoch 156/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1541 - accuracy: 0.9062\n",
      "Epoch 156: val_loss improved from 0.52375 to 0.52341, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1766 - accuracy: 0.9375 - val_loss: 0.5234 - val_accuracy: 0.7000\n",
      "Epoch 157/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2051 - accuracy: 0.9375\n",
      "Epoch 157: val_loss improved from 0.52341 to 0.51717, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1823 - accuracy: 0.9250 - val_loss: 0.5172 - val_accuracy: 0.7000\n",
      "Epoch 158/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1680 - accuracy: 0.9062\n",
      "Epoch 158: val_loss did not improve from 0.51717\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1809 - accuracy: 0.9250 - val_loss: 0.5190 - val_accuracy: 0.7000\n",
      "Epoch 159/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2182 - accuracy: 0.9375\n",
      "Epoch 159: val_loss did not improve from 0.51717\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1756 - accuracy: 0.9500 - val_loss: 0.5304 - val_accuracy: 0.7000\n",
      "Epoch 160/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2087 - accuracy: 0.9375\n",
      "Epoch 160: val_loss did not improve from 0.51717\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1701 - accuracy: 0.9500 - val_loss: 0.5320 - val_accuracy: 0.7000\n",
      "Epoch 161/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1688 - accuracy: 0.9062\n",
      "Epoch 161: val_loss did not improve from 0.51717\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1756 - accuracy: 0.9125 - val_loss: 0.5313 - val_accuracy: 0.7000\n",
      "Epoch 162/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1759 - accuracy: 0.9062\n",
      "Epoch 162: val_loss did not improve from 0.51717\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1762 - accuracy: 0.9250 - val_loss: 0.5313 - val_accuracy: 0.7000\n",
      "Epoch 163/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1467 - accuracy: 0.9375\n",
      "Epoch 163: val_loss did not improve from 0.51717\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2076 - accuracy: 0.9250 - val_loss: 0.5244 - val_accuracy: 0.7000\n",
      "Epoch 164/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1100 - accuracy: 0.9375\n",
      "Epoch 164: val_loss improved from 0.51717 to 0.51456, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1850 - accuracy: 0.9250 - val_loss: 0.5146 - val_accuracy: 0.7000\n",
      "Epoch 165/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1031 - accuracy: 1.0000\n",
      "Epoch 165: val_loss improved from 0.51456 to 0.50672, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1923 - accuracy: 0.9250 - val_loss: 0.5067 - val_accuracy: 0.7000\n",
      "Epoch 166/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1992 - accuracy: 0.9375\n",
      "Epoch 166: val_loss improved from 0.50672 to 0.50511, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1939 - accuracy: 0.9125 - val_loss: 0.5051 - val_accuracy: 0.7000\n",
      "Epoch 167/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1961 - accuracy: 0.9062\n",
      "Epoch 167: val_loss improved from 0.50511 to 0.50057, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1934 - accuracy: 0.9125 - val_loss: 0.5006 - val_accuracy: 0.7000\n",
      "Epoch 168/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1207 - accuracy: 0.9688\n",
      "Epoch 168: val_loss did not improve from 0.50057\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1791 - accuracy: 0.9125 - val_loss: 0.5031 - val_accuracy: 0.7000\n",
      "Epoch 169/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1834 - accuracy: 0.9688\n",
      "Epoch 169: val_loss did not improve from 0.50057\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1758 - accuracy: 0.9500 - val_loss: 0.5100 - val_accuracy: 0.7000\n",
      "Epoch 170/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1692 - accuracy: 0.9062\n",
      "Epoch 170: val_loss did not improve from 0.50057\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1933 - accuracy: 0.9000 - val_loss: 0.5216 - val_accuracy: 0.7000\n",
      "Epoch 171/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1586 - accuracy: 0.9688\n",
      "Epoch 171: val_loss did not improve from 0.50057\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1564 - accuracy: 0.9500 - val_loss: 0.5184 - val_accuracy: 0.7000\n",
      "Epoch 172/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1769 - accuracy: 0.9375\n",
      "Epoch 172: val_loss did not improve from 0.50057\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1833 - accuracy: 0.9250 - val_loss: 0.5077 - val_accuracy: 0.7000\n",
      "Epoch 173/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1925 - accuracy: 0.8750\n",
      "Epoch 173: val_loss improved from 0.50057 to 0.49687, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1780 - accuracy: 0.9250 - val_loss: 0.4969 - val_accuracy: 0.7000\n",
      "Epoch 174/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2785 - accuracy: 0.9062\n",
      "Epoch 174: val_loss improved from 0.49687 to 0.49049, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1727 - accuracy: 0.9500 - val_loss: 0.4905 - val_accuracy: 0.7000\n",
      "Epoch 175/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1708 - accuracy: 0.9375\n",
      "Epoch 175: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1738 - accuracy: 0.9500 - val_loss: 0.4997 - val_accuracy: 0.7000\n",
      "Epoch 176/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1861 - accuracy: 0.9375\n",
      "Epoch 176: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1649 - accuracy: 0.9625 - val_loss: 0.5029 - val_accuracy: 0.7000\n",
      "Epoch 177/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1424 - accuracy: 0.9688\n",
      "Epoch 177: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1548 - accuracy: 0.9625 - val_loss: 0.5026 - val_accuracy: 0.7000\n",
      "Epoch 178/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1750 - accuracy: 0.9062\n",
      "Epoch 178: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1582 - accuracy: 0.9250 - val_loss: 0.4980 - val_accuracy: 0.7000\n",
      "Epoch 179/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1639 - accuracy: 0.9375\n",
      "Epoch 179: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1704 - accuracy: 0.9375 - val_loss: 0.5001 - val_accuracy: 0.7000\n",
      "Epoch 180/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1252 - accuracy: 0.9375\n",
      "Epoch 180: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1714 - accuracy: 0.9000 - val_loss: 0.5062 - val_accuracy: 0.7000\n",
      "Epoch 181/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1880 - accuracy: 0.9375\n",
      "Epoch 181: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1793 - accuracy: 0.9250 - val_loss: 0.5069 - val_accuracy: 0.7000\n",
      "Epoch 182/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2238 - accuracy: 0.9062\n",
      "Epoch 182: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1762 - accuracy: 0.9250 - val_loss: 0.5179 - val_accuracy: 0.7000\n",
      "Epoch 183/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1894 - accuracy: 0.9375\n",
      "Epoch 183: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1729 - accuracy: 0.9500 - val_loss: 0.5190 - val_accuracy: 0.7000\n",
      "Epoch 184/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1682 - accuracy: 0.9375\n",
      "Epoch 184: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1520 - accuracy: 0.9500 - val_loss: 0.5134 - val_accuracy: 0.7000\n",
      "Epoch 185/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1837 - accuracy: 0.9375\n",
      "Epoch 185: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1562 - accuracy: 0.9750 - val_loss: 0.5112 - val_accuracy: 0.7000\n",
      "Epoch 186/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0839 - accuracy: 1.0000\n",
      "Epoch 186: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1443 - accuracy: 0.9625 - val_loss: 0.5104 - val_accuracy: 0.7000\n",
      "Epoch 187/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1970 - accuracy: 0.9062\n",
      "Epoch 187: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1564 - accuracy: 0.9375 - val_loss: 0.5125 - val_accuracy: 0.7000\n",
      "Epoch 188/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1206 - accuracy: 1.0000\n",
      "Epoch 188: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1607 - accuracy: 0.9625 - val_loss: 0.5127 - val_accuracy: 0.7000\n",
      "Epoch 189/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1988 - accuracy: 0.9062\n",
      "Epoch 189: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1726 - accuracy: 0.9250 - val_loss: 0.5040 - val_accuracy: 0.7000\n",
      "Epoch 190/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1553 - accuracy: 0.9688\n",
      "Epoch 190: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1740 - accuracy: 0.9250 - val_loss: 0.4974 - val_accuracy: 0.7000\n",
      "Epoch 191/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1013 - accuracy: 0.9688\n",
      "Epoch 191: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1729 - accuracy: 0.9375 - val_loss: 0.4947 - val_accuracy: 0.7000\n",
      "Epoch 192/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1621 - accuracy: 0.9688\n",
      "Epoch 192: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1506 - accuracy: 0.9625 - val_loss: 0.4958 - val_accuracy: 0.7000\n",
      "Epoch 193/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1691 - accuracy: 0.8750\n",
      "Epoch 193: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1876 - accuracy: 0.9250 - val_loss: 0.4992 - val_accuracy: 0.7000\n",
      "Epoch 194/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2504 - accuracy: 0.9062\n",
      "Epoch 194: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1572 - accuracy: 0.9375 - val_loss: 0.5110 - val_accuracy: 0.7000\n",
      "Epoch 195/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1614 - accuracy: 0.9375\n",
      "Epoch 195: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1471 - accuracy: 0.9500 - val_loss: 0.5324 - val_accuracy: 0.7000\n",
      "Epoch 196/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1809 - accuracy: 0.9375\n",
      "Epoch 196: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1691 - accuracy: 0.9375 - val_loss: 0.5423 - val_accuracy: 0.7000\n",
      "Epoch 197/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1371 - accuracy: 1.0000\n",
      "Epoch 197: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1488 - accuracy: 0.9625 - val_loss: 0.5363 - val_accuracy: 0.7000\n",
      "Epoch 198/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2191 - accuracy: 0.9375\n",
      "Epoch 198: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1721 - accuracy: 0.9500 - val_loss: 0.5360 - val_accuracy: 0.7000\n",
      "Epoch 199/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0834 - accuracy: 1.0000\n",
      "Epoch 199: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1765 - accuracy: 0.9500 - val_loss: 0.5333 - val_accuracy: 0.7000\n",
      "Epoch 200/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1163 - accuracy: 0.9688\n",
      "Epoch 200: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1476 - accuracy: 0.9625 - val_loss: 0.5181 - val_accuracy: 0.7000\n",
      "Epoch 201/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1113 - accuracy: 0.9688\n",
      "Epoch 201: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1506 - accuracy: 0.9500 - val_loss: 0.5126 - val_accuracy: 0.7000\n",
      "Epoch 202/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1782 - accuracy: 0.9375\n",
      "Epoch 202: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1633 - accuracy: 0.9500 - val_loss: 0.5093 - val_accuracy: 0.7000\n",
      "Epoch 203/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1979 - accuracy: 0.9688\n",
      "Epoch 203: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1495 - accuracy: 0.9750 - val_loss: 0.4972 - val_accuracy: 0.7000\n",
      "Epoch 204/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1186 - accuracy: 1.0000\n",
      "Epoch 204: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1542 - accuracy: 0.9750 - val_loss: 0.4938 - val_accuracy: 0.7000\n",
      "Epoch 205/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1548 - accuracy: 0.9375\n",
      "Epoch 205: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1401 - accuracy: 0.9625 - val_loss: 0.4939 - val_accuracy: 0.7000\n",
      "Epoch 206/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1079 - accuracy: 1.0000\n",
      "Epoch 206: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1348 - accuracy: 0.9625 - val_loss: 0.4946 - val_accuracy: 0.7000\n",
      "Epoch 207/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1968 - accuracy: 0.9375\n",
      "Epoch 207: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1645 - accuracy: 0.9500 - val_loss: 0.4945 - val_accuracy: 0.7000\n",
      "Epoch 208/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0952 - accuracy: 0.9688\n",
      "Epoch 208: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1619 - accuracy: 0.9375 - val_loss: 0.5013 - val_accuracy: 0.7000\n",
      "Epoch 209/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1688 - accuracy: 0.9062\n",
      "Epoch 209: val_loss did not improve from 0.49049\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1529 - accuracy: 0.9375 - val_loss: 0.4930 - val_accuracy: 0.7000\n",
      "Epoch 210/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1167 - accuracy: 0.9688\n",
      "Epoch 210: val_loss improved from 0.49049 to 0.48729, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1562 - accuracy: 0.9375 - val_loss: 0.4873 - val_accuracy: 0.7000\n",
      "Epoch 211/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1724 - accuracy: 0.9688\n",
      "Epoch 211: val_loss did not improve from 0.48729\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1491 - accuracy: 0.9750 - val_loss: 0.4953 - val_accuracy: 0.7000\n",
      "Epoch 212/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1492 - accuracy: 0.9688\n",
      "Epoch 212: val_loss did not improve from 0.48729\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1415 - accuracy: 0.9750 - val_loss: 0.4968 - val_accuracy: 0.7000\n",
      "Epoch 213/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1617 - accuracy: 0.9375\n",
      "Epoch 213: val_loss did not improve from 0.48729\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1391 - accuracy: 0.9750 - val_loss: 0.4957 - val_accuracy: 0.7000\n",
      "Epoch 214/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1615 - accuracy: 0.9688\n",
      "Epoch 214: val_loss did not improve from 0.48729\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1582 - accuracy: 0.9625 - val_loss: 0.5007 - val_accuracy: 0.7000\n",
      "Epoch 215/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1551 - accuracy: 0.9688\n",
      "Epoch 215: val_loss did not improve from 0.48729\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1421 - accuracy: 0.9750 - val_loss: 0.5017 - val_accuracy: 0.7000\n",
      "Epoch 216/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1893 - accuracy: 0.9375\n",
      "Epoch 216: val_loss did not improve from 0.48729\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1555 - accuracy: 0.9500 - val_loss: 0.4888 - val_accuracy: 0.7000\n",
      "Epoch 217/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1431 - accuracy: 0.9375\n",
      "Epoch 217: val_loss improved from 0.48729 to 0.48173, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1550 - accuracy: 0.9500 - val_loss: 0.4817 - val_accuracy: 0.7000\n",
      "Epoch 218/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0922 - accuracy: 0.9688\n",
      "Epoch 218: val_loss improved from 0.48173 to 0.48067, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1461 - accuracy: 0.9500 - val_loss: 0.4807 - val_accuracy: 0.7000\n",
      "Epoch 219/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1727 - accuracy: 0.9375\n",
      "Epoch 219: val_loss improved from 0.48067 to 0.47146, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1351 - accuracy: 0.9500 - val_loss: 0.4715 - val_accuracy: 0.7000\n",
      "Epoch 220/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1395 - accuracy: 0.9688\n",
      "Epoch 220: val_loss improved from 0.47146 to 0.46799, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1474 - accuracy: 0.9625 - val_loss: 0.4680 - val_accuracy: 0.7000\n",
      "Epoch 221/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1580 - accuracy: 0.9375\n",
      "Epoch 221: val_loss improved from 0.46799 to 0.46774, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1496 - accuracy: 0.9500 - val_loss: 0.4677 - val_accuracy: 0.7000\n",
      "Epoch 222/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1226 - accuracy: 0.9688\n",
      "Epoch 222: val_loss improved from 0.46774 to 0.46686, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1373 - accuracy: 0.9625 - val_loss: 0.4669 - val_accuracy: 0.7000\n",
      "Epoch 223/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0864 - accuracy: 1.0000\n",
      "Epoch 223: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1457 - accuracy: 0.9625 - val_loss: 0.4788 - val_accuracy: 0.7000\n",
      "Epoch 224/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0914 - accuracy: 0.9688\n",
      "Epoch 224: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1525 - accuracy: 0.9500 - val_loss: 0.4953 - val_accuracy: 0.7000\n",
      "Epoch 225/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1884 - accuracy: 0.9688\n",
      "Epoch 225: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1516 - accuracy: 0.9500 - val_loss: 0.5128 - val_accuracy: 0.7500\n",
      "Epoch 226/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1809 - accuracy: 0.9375\n",
      "Epoch 226: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1489 - accuracy: 0.9375 - val_loss: 0.5276 - val_accuracy: 0.7500\n",
      "Epoch 227/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2011 - accuracy: 0.9062\n",
      "Epoch 227: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1580 - accuracy: 0.9375 - val_loss: 0.5111 - val_accuracy: 0.7500\n",
      "Epoch 228/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1494 - accuracy: 0.9375\n",
      "Epoch 228: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1407 - accuracy: 0.9625 - val_loss: 0.4892 - val_accuracy: 0.7000\n",
      "Epoch 229/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0887 - accuracy: 0.9688\n",
      "Epoch 229: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1332 - accuracy: 0.9625 - val_loss: 0.4779 - val_accuracy: 0.7000\n",
      "Epoch 230/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1120 - accuracy: 0.9688\n",
      "Epoch 230: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1606 - accuracy: 0.9625 - val_loss: 0.4739 - val_accuracy: 0.7000\n",
      "Epoch 231/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1014 - accuracy: 1.0000\n",
      "Epoch 231: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1355 - accuracy: 0.9750 - val_loss: 0.4791 - val_accuracy: 0.7000\n",
      "Epoch 232/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1610 - accuracy: 0.9688\n",
      "Epoch 232: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1493 - accuracy: 0.9500 - val_loss: 0.4888 - val_accuracy: 0.7000\n",
      "Epoch 233/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1565 - accuracy: 0.9375\n",
      "Epoch 233: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1356 - accuracy: 0.9625 - val_loss: 0.4880 - val_accuracy: 0.7000\n",
      "Epoch 234/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1286 - accuracy: 0.9688\n",
      "Epoch 234: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1307 - accuracy: 0.9625 - val_loss: 0.4883 - val_accuracy: 0.7000\n",
      "Epoch 235/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0986 - accuracy: 1.0000\n",
      "Epoch 235: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1509 - accuracy: 0.9625 - val_loss: 0.4983 - val_accuracy: 0.7000\n",
      "Epoch 236/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1941 - accuracy: 0.9375\n",
      "Epoch 236: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1407 - accuracy: 0.9625 - val_loss: 0.4995 - val_accuracy: 0.7000\n",
      "Epoch 237/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1887 - accuracy: 0.9375\n",
      "Epoch 237: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1423 - accuracy: 0.9500 - val_loss: 0.4964 - val_accuracy: 0.7000\n",
      "Epoch 238/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1094 - accuracy: 0.9688\n",
      "Epoch 238: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1465 - accuracy: 0.9500 - val_loss: 0.4854 - val_accuracy: 0.7000\n",
      "Epoch 239/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1446 - accuracy: 0.9375\n",
      "Epoch 239: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1270 - accuracy: 0.9625 - val_loss: 0.4785 - val_accuracy: 0.7000\n",
      "Epoch 240/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1362 - accuracy: 0.9688\n",
      "Epoch 240: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1333 - accuracy: 0.9750 - val_loss: 0.4673 - val_accuracy: 0.7000\n",
      "Epoch 241/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1375 - accuracy: 0.9375\n",
      "Epoch 241: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1258 - accuracy: 0.9625 - val_loss: 0.4721 - val_accuracy: 0.7000\n",
      "Epoch 242/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1692 - accuracy: 0.9375\n",
      "Epoch 242: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1433 - accuracy: 0.9500 - val_loss: 0.4761 - val_accuracy: 0.7000\n",
      "Epoch 243/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1219 - accuracy: 0.9375\n",
      "Epoch 243: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1267 - accuracy: 0.9625 - val_loss: 0.4890 - val_accuracy: 0.7000\n",
      "Epoch 244/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0908 - accuracy: 1.0000\n",
      "Epoch 244: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1269 - accuracy: 0.9750 - val_loss: 0.5095 - val_accuracy: 0.7000\n",
      "Epoch 245/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1887 - accuracy: 0.9062\n",
      "Epoch 245: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1453 - accuracy: 0.9500 - val_loss: 0.5192 - val_accuracy: 0.7500\n",
      "Epoch 246/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2171 - accuracy: 0.9375\n",
      "Epoch 246: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1311 - accuracy: 0.9750 - val_loss: 0.5145 - val_accuracy: 0.7000\n",
      "Epoch 247/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1956 - accuracy: 0.9688\n",
      "Epoch 247: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1298 - accuracy: 0.9875 - val_loss: 0.5045 - val_accuracy: 0.7000\n",
      "Epoch 248/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1577 - accuracy: 0.9688\n",
      "Epoch 248: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1326 - accuracy: 0.9750 - val_loss: 0.4891 - val_accuracy: 0.7000\n",
      "Epoch 249/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1219 - accuracy: 0.9688\n",
      "Epoch 249: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1292 - accuracy: 0.9500 - val_loss: 0.4760 - val_accuracy: 0.7000\n",
      "Epoch 250/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1388 - accuracy: 0.9688\n",
      "Epoch 250: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1365 - accuracy: 0.9750 - val_loss: 0.4704 - val_accuracy: 0.7000\n",
      "Epoch 251/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1157 - accuracy: 0.9688\n",
      "Epoch 251: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1260 - accuracy: 0.9500 - val_loss: 0.4768 - val_accuracy: 0.7000\n",
      "Epoch 252/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1107 - accuracy: 1.0000\n",
      "Epoch 252: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1209 - accuracy: 0.9750 - val_loss: 0.4865 - val_accuracy: 0.7000\n",
      "Epoch 253/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0945 - accuracy: 0.9688\n",
      "Epoch 253: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1234 - accuracy: 0.9750 - val_loss: 0.4947 - val_accuracy: 0.7000\n",
      "Epoch 254/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1727 - accuracy: 0.9688\n",
      "Epoch 254: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1390 - accuracy: 0.9625 - val_loss: 0.4862 - val_accuracy: 0.7000\n",
      "Epoch 255/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0714 - accuracy: 1.0000\n",
      "Epoch 255: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1255 - accuracy: 0.9625 - val_loss: 0.4825 - val_accuracy: 0.7000\n",
      "Epoch 256/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1127 - accuracy: 0.9688\n",
      "Epoch 256: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1313 - accuracy: 0.9625 - val_loss: 0.4772 - val_accuracy: 0.7000\n",
      "Epoch 257/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1610 - accuracy: 0.9688\n",
      "Epoch 257: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1367 - accuracy: 0.9625 - val_loss: 0.4844 - val_accuracy: 0.7000\n",
      "Epoch 258/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1903 - accuracy: 0.9688\n",
      "Epoch 258: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1276 - accuracy: 0.9875 - val_loss: 0.4826 - val_accuracy: 0.7000\n",
      "Epoch 259/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1255 - accuracy: 0.9688\n",
      "Epoch 259: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1358 - accuracy: 0.9500 - val_loss: 0.4876 - val_accuracy: 0.7000\n",
      "Epoch 260/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1264 - accuracy: 0.9688\n",
      "Epoch 260: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1325 - accuracy: 0.9625 - val_loss: 0.4876 - val_accuracy: 0.7000\n",
      "Epoch 261/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0831 - accuracy: 1.0000\n",
      "Epoch 261: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1301 - accuracy: 0.9625 - val_loss: 0.4794 - val_accuracy: 0.7000\n",
      "Epoch 262/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1605 - accuracy: 0.9375\n",
      "Epoch 262: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1124 - accuracy: 0.9625 - val_loss: 0.4734 - val_accuracy: 0.7000\n",
      "Epoch 263/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1409 - accuracy: 0.9688\n",
      "Epoch 263: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1292 - accuracy: 0.9750 - val_loss: 0.4738 - val_accuracy: 0.7000\n",
      "Epoch 264/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0967 - accuracy: 0.9688\n",
      "Epoch 264: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1343 - accuracy: 0.9625 - val_loss: 0.4812 - val_accuracy: 0.7000\n",
      "Epoch 265/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0608 - accuracy: 1.0000\n",
      "Epoch 265: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1027 - accuracy: 0.9750 - val_loss: 0.4756 - val_accuracy: 0.7000\n",
      "Epoch 266/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1015 - accuracy: 0.9688\n",
      "Epoch 266: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1205 - accuracy: 0.9750 - val_loss: 0.4739 - val_accuracy: 0.7000\n",
      "Epoch 267/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1339 - accuracy: 0.9688\n",
      "Epoch 267: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1248 - accuracy: 0.9750 - val_loss: 0.4677 - val_accuracy: 0.7000\n",
      "Epoch 268/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1027 - accuracy: 1.0000\n",
      "Epoch 268: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1271 - accuracy: 0.9625 - val_loss: 0.4725 - val_accuracy: 0.7000\n",
      "Epoch 269/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1512 - accuracy: 0.9688\n",
      "Epoch 269: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1046 - accuracy: 0.9750 - val_loss: 0.4700 - val_accuracy: 0.7000\n",
      "Epoch 270/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0749 - accuracy: 1.0000\n",
      "Epoch 270: val_loss did not improve from 0.46686\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1221 - accuracy: 0.9750 - val_loss: 0.4727 - val_accuracy: 0.7000\n",
      "Epoch 271/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0810 - accuracy: 0.9688\n",
      "Epoch 271: val_loss improved from 0.46686 to 0.45827, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1253 - accuracy: 0.9375 - val_loss: 0.4583 - val_accuracy: 0.7000\n",
      "Epoch 272/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0894 - accuracy: 1.0000\n",
      "Epoch 272: val_loss improved from 0.45827 to 0.45638, saving model to best-model.h5\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1150 - accuracy: 0.9500 - val_loss: 0.4564 - val_accuracy: 0.7000\n",
      "Epoch 273/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1074 - accuracy: 0.9375\n",
      "Epoch 273: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1209 - accuracy: 0.9625 - val_loss: 0.4701 - val_accuracy: 0.7000\n",
      "Epoch 274/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0771 - accuracy: 1.0000\n",
      "Epoch 274: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1257 - accuracy: 0.9625 - val_loss: 0.4830 - val_accuracy: 0.7000\n",
      "Epoch 275/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1057 - accuracy: 0.9688\n",
      "Epoch 275: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1090 - accuracy: 0.9750 - val_loss: 0.4940 - val_accuracy: 0.7000\n",
      "Epoch 276/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1011 - accuracy: 1.0000\n",
      "Epoch 276: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1263 - accuracy: 0.9750 - val_loss: 0.4937 - val_accuracy: 0.7000\n",
      "Epoch 277/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1410 - accuracy: 0.9375\n",
      "Epoch 277: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1328 - accuracy: 0.9375 - val_loss: 0.4829 - val_accuracy: 0.7000\n",
      "Epoch 278/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1552 - accuracy: 0.9375\n",
      "Epoch 278: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1212 - accuracy: 0.9625 - val_loss: 0.4674 - val_accuracy: 0.7000\n",
      "Epoch 279/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1847 - accuracy: 0.9375\n",
      "Epoch 279: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1171 - accuracy: 0.9750 - val_loss: 0.4629 - val_accuracy: 0.7000\n",
      "Epoch 280/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1053 - accuracy: 0.9688\n",
      "Epoch 280: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1203 - accuracy: 0.9625 - val_loss: 0.4616 - val_accuracy: 0.7000\n",
      "Epoch 281/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1379 - accuracy: 0.9688\n",
      "Epoch 281: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1185 - accuracy: 0.9750 - val_loss: 0.4669 - val_accuracy: 0.7000\n",
      "Epoch 282/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1475 - accuracy: 0.9688\n",
      "Epoch 282: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1287 - accuracy: 0.9625 - val_loss: 0.4771 - val_accuracy: 0.7000\n",
      "Epoch 283/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1015 - accuracy: 1.0000\n",
      "Epoch 283: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1122 - accuracy: 0.9750 - val_loss: 0.4905 - val_accuracy: 0.7000\n",
      "Epoch 284/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1261 - accuracy: 0.9375\n",
      "Epoch 284: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1230 - accuracy: 0.9625 - val_loss: 0.4872 - val_accuracy: 0.7000\n",
      "Epoch 285/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1387 - accuracy: 0.9375\n",
      "Epoch 285: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1236 - accuracy: 0.9625 - val_loss: 0.4761 - val_accuracy: 0.7000\n",
      "Epoch 286/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1755 - accuracy: 0.9375\n",
      "Epoch 286: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1045 - accuracy: 0.9750 - val_loss: 0.4688 - val_accuracy: 0.7000\n",
      "Epoch 287/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0988 - accuracy: 0.9688\n",
      "Epoch 287: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1072 - accuracy: 0.9625 - val_loss: 0.4683 - val_accuracy: 0.7000\n",
      "Epoch 288/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1527 - accuracy: 0.9062\n",
      "Epoch 288: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1173 - accuracy: 0.9500 - val_loss: 0.4609 - val_accuracy: 0.7000\n",
      "Epoch 289/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1017 - accuracy: 0.9688\n",
      "Epoch 289: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1086 - accuracy: 0.9750 - val_loss: 0.4597 - val_accuracy: 0.7000\n",
      "Epoch 290/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1110 - accuracy: 0.9688\n",
      "Epoch 290: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1132 - accuracy: 0.9625 - val_loss: 0.4621 - val_accuracy: 0.7000\n",
      "Epoch 291/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0617 - accuracy: 1.0000\n",
      "Epoch 291: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1110 - accuracy: 0.9625 - val_loss: 0.4723 - val_accuracy: 0.7000\n",
      "Epoch 292/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0856 - accuracy: 0.9688\n",
      "Epoch 292: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0932 - accuracy: 0.9750 - val_loss: 0.4752 - val_accuracy: 0.7000\n",
      "Epoch 293/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1037 - accuracy: 1.0000\n",
      "Epoch 293: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1011 - accuracy: 0.9875 - val_loss: 0.4799 - val_accuracy: 0.7000\n",
      "Epoch 294/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1614 - accuracy: 0.9688\n",
      "Epoch 294: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1078 - accuracy: 0.9750 - val_loss: 0.4781 - val_accuracy: 0.7000\n",
      "Epoch 295/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1588 - accuracy: 0.9375\n",
      "Epoch 295: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1208 - accuracy: 0.9750 - val_loss: 0.4771 - val_accuracy: 0.7000\n",
      "Epoch 296/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0508 - accuracy: 1.0000\n",
      "Epoch 296: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0945 - accuracy: 0.9625 - val_loss: 0.4859 - val_accuracy: 0.7000\n",
      "Epoch 297/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1246 - accuracy: 0.9688\n",
      "Epoch 297: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0982 - accuracy: 0.9750 - val_loss: 0.4847 - val_accuracy: 0.7000\n",
      "Epoch 298/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0702 - accuracy: 1.0000\n",
      "Epoch 298: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0934 - accuracy: 0.9750 - val_loss: 0.4896 - val_accuracy: 0.7000\n",
      "Epoch 299/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0698 - accuracy: 0.9688\n",
      "Epoch 299: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1032 - accuracy: 0.9500 - val_loss: 0.4821 - val_accuracy: 0.7000\n",
      "Epoch 300/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1553 - accuracy: 0.9062\n",
      "Epoch 300: val_loss did not improve from 0.45638\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1006 - accuracy: 0.9625 - val_loss: 0.4690 - val_accuracy: 0.7000\n"
     ]
    }
   ],
   "execution_count": 509
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T19:06:45.549106Z",
     "start_time": "2024-05-12T19:06:45.408027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Affichage des résultats de l'entraînement\n",
    "history_dict = history_mlp.history\n",
    "loss_train_epochs = history_dict['loss']\n",
    "loss_val_epochs = history_dict['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_train_epochs, color='blue', label='train_loss')\n",
    "plt.plot(loss_val_epochs, color='red', label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.savefig('epoch-loss.pdf')\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "id": "d352e2f350dcd0cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 510
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T19:06:45.674445Z",
     "start_time": "2024-05-12T19:06:45.549106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test de précision du modèle à l'aide du jeu de test\n",
    "model = tf.keras.models.load_model('best-model.h5')\n",
    "loss, acc = model.evaluate(X_test_array, y_test_array)\n",
    "\n",
    "print(\"Accuracy MLP:\", acc)\n",
    "print(\"Loss MLP:\", loss)"
   ],
   "id": "dfea4866629c198c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 817us/step - loss: 0.4478 - accuracy: 0.7800\n",
      "Accuracy MLP: 0.7799999713897705\n",
      "Loss MLP: 0.44782310724258423\n"
     ]
    }
   ],
   "execution_count": 511
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
